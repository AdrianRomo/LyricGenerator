{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.9.2 64-bit ('venv': venv)"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.9.2","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"interpreter":{"hash":"e725f9b5fce15d787818dde76bec417195597a92fe936e495a973f01d5d9c616"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Pop Lyric Generator"],"metadata":{}},{"cell_type":"markdown","source":["Text Generation is a type of Language Modelling problem.\n","\n","Language Modelling is the core problem for a number of Natural Language Processing tasks such as speech to text, conversational system, and text summarization, etc.\n","\n","Text Generation is a task which can be be architectured using deep learning models, particularly Recurrent Neural Networks.\n","\n","In this project, we will experiment to generate new pop lyrics based on [this Dataset](https://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres)."],"metadata":{}},{"cell_type":"markdown","source":["# Import Libraries\n","\n","We have to import the libraries we're going to use, the most relevant are:\n","\n","- **Pandas** in order to manipulate and analyze the data\n","- **Matplotlib** to visualize the data in graphics\n","- **Wordcloud** to help us when we are going to generate the next word of a sentence\n","- **Tensorflow & Keras** will help us in the process of machine learning and keras for deep learning.\n"],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import string, os \n","import tensorflow as tf\n","\n","# keras module for building LSTM \n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, Dropout, LSTM, Dense, Bidirectional \n","from keras.preprocessing.text import Tokenizer\n","from keras.callbacks import EarlyStopping\n","from keras.models import Sequential"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:24.676075Z","iopub.execute_input":"2021-09-24T00:59:24.676504Z","iopub.status.idle":"2021-09-24T00:59:29.71467Z","shell.execute_reply.started":"2021-09-24T00:59:24.676392Z","shell.execute_reply":"2021-09-24T00:59:29.71377Z"},"trusted":true}},{"cell_type":"markdown","source":["# Loading the Dataset\n","Once the Data was precleaned, it's going to be imported to the cloud. As it's mentioned in the documentation, it's on .csv because is easier to manipulate the data using Pandas library.\n"],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["# Import csv file\n","df = pd.read_csv(os.path.abspath(('/input/pop-lyrics/Pop.csv')))"],"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'os' has no attribute 'abspath'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/var/folders/4w/w35njdc57ks6m_h1xf1xg6500000gn/T/ipykernel_35487/469434168.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/input/pop-lyrics/Pop.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'abspath'"]}],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:29.716143Z","iopub.execute_input":"2021-09-24T00:59:29.716494Z","iopub.status.idle":"2021-09-24T00:59:30.584689Z","shell.execute_reply.started":"2021-09-24T00:59:29.716437Z","shell.execute_reply":"2021-09-24T00:59:30.583801Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Print first 10 rows to confirm is the right dataset\n","df.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:30.586479Z","iopub.execute_input":"2021-09-24T00:59:30.586818Z","iopub.status.idle":"2021-09-24T00:59:30.614138Z","shell.execute_reply.started":"2021-09-24T00:59:30.58679Z","shell.execute_reply":"2021-09-24T00:59:30.613106Z"},"trusted":true}},{"cell_type":"markdown","source":["# Data Cleaning\n","\n","In this step the dataset is going to be cleaned using Pandas library to remove the columns that are not going to be used and keep only the unique column called \"Lyrics\".\n","\n","![Example of the first cleaning process](https://miro.medium.com/max/2560/1*cDBVd9wlvjAxKi0hpdQkAg.png)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Drop all the columns except Lyrics\n","df.drop(['Artist','Songs','Popularity','Genre','Genres','Idiom','ALink','SName','SLink'],axis=1,inplace=True)\n","df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:30.615873Z","iopub.execute_input":"2021-09-24T00:59:30.61623Z","iopub.status.idle":"2021-09-24T00:59:30.625277Z","shell.execute_reply.started":"2021-09-24T00:59:30.616193Z","shell.execute_reply":"2021-09-24T00:59:30.624407Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Print first 10 rows to confirm the drops were made correctly\n","df.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:30.626829Z","iopub.execute_input":"2021-09-24T00:59:30.627355Z","iopub.status.idle":"2021-09-24T00:59:30.638638Z","shell.execute_reply.started":"2021-09-24T00:59:30.627316Z","shell.execute_reply":"2021-09-24T00:59:30.637541Z"},"trusted":true}},{"cell_type":"markdown","source":["# Shaping the Dataset\n","Since kaggle does not have enough memory (16gb RAM), disk usage (73 GB) and GPU (13GB) to process the entire dataset, we will have to limit our model from 28441 to 700 lyrics. Kaggle give It's worth mentioning that this same test was performed locally and took approximately 100 hours to perform."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Dataframe shape to show how many columns and rows the dataframe have\n","df.shape"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:30.639978Z","iopub.execute_input":"2021-09-24T00:59:30.640325Z","iopub.status.idle":"2021-09-24T00:59:30.647768Z","shell.execute_reply.started":"2021-09-24T00:59:30.640288Z","shell.execute_reply":"2021-09-24T00:59:30.646559Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Take first 700 rows\n","df = df[:700]"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:30.649305Z","iopub.execute_input":"2021-09-24T00:59:30.649683Z","iopub.status.idle":"2021-09-24T00:59:30.654817Z","shell.execute_reply.started":"2021-09-24T00:59:30.649647Z","shell.execute_reply":"2021-09-24T00:59:30.653866Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["df.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:30.657217Z","iopub.execute_input":"2021-09-24T00:59:30.657619Z","iopub.status.idle":"2021-09-24T00:59:30.667506Z","shell.execute_reply.started":"2021-09-24T00:59:30.657582Z","shell.execute_reply":"2021-09-24T00:59:30.666553Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Dataframe shape to show how many columns and rows the dataframe have after last cell \n","df.shape"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:30.669201Z","iopub.execute_input":"2021-09-24T00:59:30.669588Z","iopub.status.idle":"2021-09-24T00:59:30.676811Z","shell.execute_reply.started":"2021-09-24T00:59:30.669554Z","shell.execute_reply":"2021-09-24T00:59:30.675584Z"},"trusted":true}},{"cell_type":"markdown","source":["Now, to get Statistical information, the next cell is going to calculate the number of words per row, this is going to help us to determine the Frequency Distribution of number of words for each text extracted"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["df['Number_of_words'] = df['Lyric'].apply(lambda x:len(str(x).split()))\n","df.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:30.856905Z","iopub.execute_input":"2021-09-24T00:59:30.857225Z","iopub.status.idle":"2021-09-24T00:59:30.894277Z","shell.execute_reply.started":"2021-09-24T00:59:30.857194Z","shell.execute_reply":"2021-09-24T00:59:30.892899Z"},"trusted":true}},{"cell_type":"markdown","source":["# Statistical information"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["df['Number_of_words'].describe()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:30.919272Z","iopub.execute_input":"2021-09-24T00:59:30.919533Z","iopub.status.idle":"2021-09-24T00:59:30.931659Z","shell.execute_reply.started":"2021-09-24T00:59:30.919509Z","shell.execute_reply":"2021-09-24T00:59:30.930341Z"},"trusted":true}},{"cell_type":"markdown","source":["- **count**: Number of rows to evaluate\n","- **mean**: Average of words per row in the dataset\n","- **std**: Word's standard deviation\n","- **min**: Minimum amount of words found in a lyric or row\n","- **max**: Maximum amount of words found."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","plt.figure(figsize=(12,6))\n","sns.distplot(df['Number_of_words'],kde = False,color=\"red\",bins=200)\n","plt.title(\"Frequency distribution of number of words for each text extracted\", size=20)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:31.092348Z","iopub.execute_input":"2021-09-24T00:59:31.092667Z","iopub.status.idle":"2021-09-24T00:59:31.754148Z","shell.execute_reply.started":"2021-09-24T00:59:31.092638Z","shell.execute_reply":"2021-09-24T00:59:31.753193Z"},"trusted":true}},{"cell_type":"markdown","source":["- **Y axis**: Represent how many times a word was found in the dataset\n","- **X axis**: Represent the amount of different words found; "],"metadata":{}},{"cell_type":"markdown","source":["# Tokenization\n","\n","The data from the column named **Lyric** must be preprocessed and to achieve these, the words will have to be converted into numbers. This process is called \"tokenization\".\n","\n","Keras uses the class ___Tokenizer()___ to do this job, this class has two important methods to be used:\n","\n","- ___fit_on_text()___ : Update the internal vocabulary based on a given list of texts or in this case, \"Lyrics\" column, where each entry of the list is going to be a token.  \n","\n","- ___texts_to_sequences()___ : Transform each text within the list of texts supplied to a sequence of integers; only words known by the tokenizer will be considered.\n","\n","![Tokenization Example](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/tokenization.png)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['Lyric'].astype(str).str.lower())\n","\n","total_words = len(tokenizer.word_index)+1\n","tokenized_sentences = tokenizer.texts_to_sequences(df['Lyric'].astype(str))\n","tokenized_sentences[0]"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:31.755759Z","iopub.execute_input":"2021-09-24T00:59:31.756134Z","iopub.status.idle":"2021-09-24T00:59:32.30416Z","shell.execute_reply.started":"2021-09-24T00:59:31.756096Z","shell.execute_reply":"2021-09-24T00:59:32.303108Z"},"trusted":true}},{"cell_type":"markdown","source":["# Slash sequences into a n-gram sequence\n","After the text is tokenized, the words will be sorted to represent them numerically by creating an input sequence using the created tokens."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["input_sequences = list()\n","for i in tokenized_sentences:\n","    for t in range(1, len(i)):\n","        n_gram_sequence = i[:t+1]\n","        input_sequences.append(n_gram_sequence)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:32.306279Z","iopub.execute_input":"2021-09-24T00:59:32.306707Z","iopub.status.idle":"2021-09-24T00:59:34.937765Z","shell.execute_reply.started":"2021-09-24T00:59:32.306666Z","shell.execute_reply":"2021-09-24T00:59:34.936688Z"},"trusted":true}},{"cell_type":"markdown","source":["# Padding\n","\n","Before the model generation, is necessary to normalize all sentences to the same standard lenght, to avoid the memory overflow and to get the layers of the model way more deep, this is a simple process to add a 0's in the beginning of the text, resulting in layers of the same size.\n","\n","___pad_sequences___ Transform a list of sequences that is a lists of integers into a 2D array, in this case the list is called input_sequences.\n","\n","Sequences resulting shorter than **maxlen** are padded with 0's until they have the same length.  \n","\n","The position where the zeros will be added is determined by the argument **padding**, in this case, it will be done at the beginning of the sequence.\n","\n","![Padding Example](https://miro.medium.com/max/700/1*CPLhZoVSTCWgAxe2LKXoOA.png)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:34.940798Z","iopub.execute_input":"2021-09-24T00:59:34.94114Z","iopub.status.idle":"2021-09-24T00:59:54.535622Z","shell.execute_reply.started":"2021-09-24T00:59:34.94111Z","shell.execute_reply":"2021-09-24T00:59:54.534674Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["input_sequences[:20]"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:54.536934Z","iopub.execute_input":"2021-09-24T00:59:54.537271Z","iopub.status.idle":"2021-09-24T00:59:54.543865Z","shell.execute_reply.started":"2021-09-24T00:59:54.537234Z","shell.execute_reply":"2021-09-24T00:59:54.543078Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# create predictors and label\n","X, labels = input_sequences[:,:-1],input_sequences[:,-1]\n","y = tf.keras.utils.to_categorical(labels, num_classes=total_words)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:54.545021Z","iopub.execute_input":"2021-09-24T00:59:54.545413Z","iopub.status.idle":"2021-09-24T00:59:55.468281Z","shell.execute_reply.started":"2021-09-24T00:59:54.545378Z","shell.execute_reply":"2021-09-24T00:59:55.467384Z"},"trusted":true}},{"cell_type":"markdown","source":["# Creating the Model\n","\n","This test will use the Bidirectional LSTM model, this kind of neural networks run as the name says: in two ways. This is from past to future and vice versa, this is how the model preserves the information of both states at any point. LSTM neural networks are mostly used where context is involved.\n","\n","![Bidirectional LSTM Diagram](https://mlwhiz.com/images/birnn.png)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["model = Sequential()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:55.469631Z","iopub.execute_input":"2021-09-24T00:59:55.469975Z","iopub.status.idle":"2021-09-24T00:59:57.353105Z","shell.execute_reply.started":"2021-09-24T00:59:55.469941Z","shell.execute_reply":"2021-09-24T00:59:57.35197Z"},"trusted":true}},{"cell_type":"markdown","source":["Models in Keras are defined as a sequence of layers, and the Sequential model is about adding layers one at a time. Layers are the basic building block of neural network.  "],"metadata":{}},{"cell_type":"markdown","source":["## Layers:"],"metadata":{}},{"cell_type":"markdown","source":["### Embedding\n","- Is a core layer, can only be used as the first layer in a model, turns positive integers into dense vectors of fixed size (first parameter is the size of the vocabulary, second parameter is the dimension of the dense embedding, and third parameter is about the length of sequences which is required because we will use a Dense layer later)  "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["model.add(Embedding(total_words, 40, input_length=max_sequence_len-1))\n"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:57.359172Z","iopub.execute_input":"2021-09-24T00:59:57.359732Z","iopub.status.idle":"2021-09-24T00:59:57.411629Z","shell.execute_reply.started":"2021-09-24T00:59:57.35969Z","shell.execute_reply":"2021-09-24T00:59:57.410562Z"},"trusted":true}},{"cell_type":"markdown","source":["### Bidirectional:\n","- Is a recurrent layer, a bidirectional wrapper for RNN's which will recibe a layer as an input being LSTM layer the one we chose, it will receive a positive integer as it's input which refers to the amount of output nodes that should be returned. "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["model.add(Bidirectional(LSTM(250)))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:57.415987Z","iopub.execute_input":"2021-09-24T00:59:57.41633Z","iopub.status.idle":"2021-09-24T00:59:57.986878Z","shell.execute_reply.started":"2021-09-24T00:59:57.416297Z","shell.execute_reply":"2021-09-24T00:59:57.985971Z"},"trusted":true}},{"cell_type":"markdown","source":["### Dropout:\n","- Is a regularization layer. This layer randomly sets input units to 0, with a frequency of the value we pass it,at each step during training time which helps prevent overfitting.  "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["model.add(Dropout(0.1))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:57.988364Z","iopub.execute_input":"2021-09-24T00:59:57.988729Z","iopub.status.idle":"2021-09-24T00:59:57.9972Z","shell.execute_reply.started":"2021-09-24T00:59:57.98869Z","shell.execute_reply":"2021-09-24T00:59:57.996118Z"},"trusted":true}},{"cell_type":"markdown","source":["### Dense:\n","- Is a core layer, and a densely-connected neural network layer. Receives as first parameter a positive integer which refers to the amount of output nodes that should be returned. Second parameter is the one named **activation** which defines the type of predictions the model can make; for the kind of problem we are abording the one which suits the better is softmax which outputs a vector of values (input) that can be interpretated as probabilities of being used.  "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["model.add(Dense(total_words, activation='softmax'))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:57.998779Z","iopub.execute_input":"2021-09-24T00:59:57.999142Z","iopub.status.idle":"2021-09-24T00:59:58.016516Z","shell.execute_reply.started":"2021-09-24T00:59:57.999106Z","shell.execute_reply":"2021-09-24T00:59:58.015788Z"},"trusted":true}},{"cell_type":"markdown","source":["### Compile method.\n","Takes two relevant parameters:\n","\n","- **loss**: Also known as a cost function; works during the optimization process and it's role is to calculate the error of the model. Cross-entropy is used to estimate the difference between an esimated and predicted probability distributions. categorical_cross-entropy will be used because it's best suited for this kind of problems and is almost universally used to train deep learning neural networks due to the results it produces.  \n","- **optimizer**: Is responsible for reducing the losses and to provide the most accurate results possible. Adam is the option chosen because is the best choice offered by Keras to train the neural network in less time and more efficiently.   Earlystop will stop the training if the model has stopped improving, this will be checked at the end of every epoch."],"metadata":{}},{"cell_type":"markdown","source":["## Optimization algorithm and Performance metrics\n","\n","The optimization algorithm added in the configuration layer will be __Adam__, because of the good perfomance and results in other projects it has.\n","\n","In this case, the 'accuracy' will be measured as perfomance metric, which gives closeness of calculated value to the actual value."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:58.017619Z","iopub.execute_input":"2021-09-24T00:59:58.017963Z","iopub.status.idle":"2021-09-24T00:59:58.034326Z","shell.execute_reply.started":"2021-09-24T00:59:58.017927Z","shell.execute_reply":"2021-09-24T00:59:58.033396Z"},"trusted":true}},{"cell_type":"markdown","source":["**fit** method trains the model for the fixed number of epochs given (iterations on the dataset)."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n","history = model.fit(X, y, epochs=20, verbose=1, callbacks=[earlystop])"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-24T00:59:58.035722Z","iopub.execute_input":"2021-09-24T00:59:58.036242Z"},"trusted":true}},{"cell_type":"markdown","source":["# Evaluating the Model\n","\n","In this block a new graphic will be generated and displayed, the Y axis will stand for accuracy and X axis will stand for the amount of epochs; as shown, in order to increase the accuracy, you have to increase the number of epochs during the training. "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# plot the accuracy\n","\n","plt.plot(history.history['accuracy'], label='train acc')\n","plt.legend()\n","plt.show()\n","plt.savefig('AccVal_acc')\n"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# Import the Trained Model\n","\n","Once the training process is completed it only remains the import of our model so we can test how does it work, in our case the trained model is called 'song_lyrics_generator'."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from keras.models import load_model\n","model = load_model('../input/songlyricmodel/song_lyrics_generator.h5')"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# Function to Generate the song\n","\n","This step is in charge of preparing the function that will be used to complete a song given the model previously trained, it will predict the next words based on the input words suministrated as 'seed_text'. For this to work a tokenization must be applied to the seed_text, then a padding will be applied to the sequences generated and passed into the trained model so the next word can be predicted."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def complete_this_song(seed_text, next_words):\n","    for _ in range(next_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","        predicted = model.predict_classes(token_list, verbose=0)        \n","        output_word = \"\"\n","        for word, index in tokenizer.word_index.items():\n","            if index == predicted:\n","                output_word = word\n","                break\n","        seed_text += \" \" + output_word\n","    return seed_text"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# Examples"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["complete_this_song(\"I am missing you\", 200)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["complete_this_song(\"It's a cruel and random world\", 80)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["complete_this_song(\"I want a piece of pizza\", 80)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["complete_this_song(\"Love flowers\", 80)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["complete_this_song(\"Hate flowers\", 80)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}